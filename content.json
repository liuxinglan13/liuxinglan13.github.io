{"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"标签","text":"","link":"/tags/index.html"}],"posts":[{"title":"DJango部署 Gunicorn+Nginx+Django+supervisor+Virtualenv","text":"这两天部署了一下自己写的django blog 记录一下备忘 系统：Ubuntu 16.04.2 部署方案：Gunicorn+Nginx+Django+supervisor+Virtualenv virtualenv 创建一个python3环境用于运行django blog 安装相关软件1234567apt-get install nginx //安装nginxsystemctl enable nginx //设置nginx开机自启动apt-getinstall python3 python3-pip //安装python3和PIPpython3-m pip install virtualenv //安装virtualenv 创建项目环境1234567891011mkdir /django //dango相关主目录mkdir /django/site //用来放网站文件mkdir /django/env //用来放虚拟环境目录创建虚拟环境cd/django/envvirtualenv --python=python3 django //这个用来跑django项目 拉取django项目文件并安装项目依赖1234567891011121314151617cd/django/sitegit cloneXXXXXXXX(你的项目git仓库地址) //拉取项目cd Django-project（你的Django项目）source /django/env/django/bin/activate //激活虚拟环境pip install -r requirements //安装项目依赖这步不懂的自行百度pip安装完毕依赖后python manager.py runserver //测试一下项目和环境在本机能正常访问127.0.0.1:8000表示正常pip install gunicorn //在虚拟环境下安装gunicornpip install gevent//在虚拟环境下安装gevent django相关配置1234567891011配置项目settings.pyINSTALLED_APPS下添加gunicornDEBUG= False //关闭调试模式ALLOWED_HOSTS= ['*'] //我直接设成所有了也可以根据自己需要设置STATIC_ROOT= os.path.join(BASE_DIR, 'static') //设置静态文件目录python manage.py collectstatic //收集静态文件（在django虚拟环境下执行） 测试gunicorn123456789使用gunicorn替换runserver启动django激活虚拟环境切换到Django项目根目录gunicorn --worker-class=gevent blogprject.wsgi:application//blogprject就是你的项目名称内有setting.py和wsgi.py的那个文件夹的名字无报错，能正常访问127.0.0.1:8000表示成功 配置 nginxvim /etc/nginx/sites-available/default //ubuntu下是这个配置文件（记得先备份）其他系统一般是 /etc/nginx/nginx.conf1234567891011121314151617181920212223server {listen 80; server_name 127.0.0.1:8000; access_log /var/log/nginx/django.log; location / { proxy_pass http://127.0.0.1:8000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } location /static/ { #静态文件引用所有url中带static的都指向下面的目录 root /django/site/Django-project; #Django项目所在目录 } location /media/ { #同上这个文件我的项目中是放图片和头像的 root /django/site/Django-project; }} 测试 先开启nginx 再运行 gunicorn 后 访问服务器外网IP 能正常访问–正常 配置 supervisor守护进程 作用自行百度我的理解 就是 保证 gunicorn 进程一直存在 方便启动和停止superisor 不支持python3 不过不影响使用1234apt-get install python-pip //安装python2下的pip ubuntu自带python2.7pip install supervisor //直接在ubuntu中安装 不是虚拟环境echo_supervisord_conf &gt; supervisor.conf //创建配置文件vim supervisor.conf //编辑配置文件激活web页面对比一下原配置文件去掉冒号就行了 配置要守护的进程 gunirorn说明：/django/env/django/bin/gunicorn 是虚拟目录django的路径 supervisor操作• supervisord -c supervisor.conf //启动supervisor程序• supervisorctl //进入交互式界面 输用户名密码（默认 user/123 配置文件里有）• status //查看APP状态 • start all start django //启动所有app或者启动指定• stop 同上• 测试 能否通过 supervisor 启动或停止 gunicron 设置supervisor开机自启动创建启动文件 vim /lib/systemd/system/supervisord.service12345678910111213141516171819[Unit]Description=Process Monitoring and Control DaemonAfter=rc-local.service[Service]Type=forkingExecStart=/usr/local/bin/supervisord -c /django/supervisor.conf // /django/supervisor.conf配置文件的路径最好使用绝对路径SysVStartPriority=99[Install]WantedBy=multi-user.targetsystemctl stop supervisord.servicesystemctl start supervisord.service测试能都通过上述命令 停止和启动 supervisor 如果可以表示成功systemctl enable supervisord.service 设置开机自启动因为在 配置文件中已经开启了 gunicron app 随supervisor启动后自启确保nginx也是开机自启动后，尝试 reboot 系统如重启后能正常访问页面表示成功！！ ####使用 Fabric还可以使用 fabric 进一步自动化部署更新服务器上代码的步骤一般有这么几个吧？ 到github上下载更新的代码 重新收集一下静态文件 重新启动 gunicorn 重新启动 nginx最少也要上面四个步骤吧？如果每次更新都这样来一遍页很麻烦吧？ 所以该 Fabric 这个神器出场啦。。。。","link":"/2017/09/15/Django/"},{"title":"Redis学习","text":"Redis学习 Redis部署相关记录 Reids.conf配置说明 https://jasonkayzk.github.io/2020/01/17/%E8%BD%AC-redis%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AEredis-conf%E8%AF%B4%E6%98%8E/ 集群模式 模式 描述 缺点 单机 哨兵模式Sentinel 配置相对简单,实现了高可用 不支持横向扩展 群集模式 分布式,分槽,支持横向扩展 只能使用DB0,部署复杂 总结,在数据量和性能要求不是特别高的时候,哨兵模式可以满足大部分需求 哨兵模式 K8S环境下使用helm3快速部署哨兵模式Redis集群 https://github.com/helm/charts/tree/master/stable/redis-ha https://www.cnblogs.com/Dev0ps/p/11259401.html 此种部署模式不适用暴露出来对K8S环境外提供服务, 因为哨兵模式下只有Master节点可以写入, 如果通过NodePort或者Treafik等将Redis暴露出去,当Master节点宕机时,客户端无法知晓新的Master, 客户端需要访问Sentinel端口26379(默认)来获取master服务器的地址, 返回的将是pod的内部地址,外部客户端将无法连接. 如果客户端在K8S集群内部则可通过内部IP连接. 部署1helm install -f values.yaml stable/redis-ha --namespace redis --generate-name 12345678910111213141516171819202122232425# values.yamlimage: repository: 172.16.130.72/basis/redis #我这里使用的是redis官方镜像 tag: latest pullPolicy: IfNotPresentredis: resources: requests: memory: 200Mi cpu: 100m limits: memory: 700Misentinel: resources: requests: memory: 200Mi cpu: 100m limits: memory: 200MipersistentVolume: enabled: true storageClass: \"nfs\" # 自己的storageClass accessModes: - ReadWriteOnce size: 10Gi -f values.yaml 指定了一个配置文件,用来替换默认的参数,也可以不指定 –namespace redis 命名空间需自己创建 –generate-name helm会自动生成一个项目名称 123helm -n redis list #部署完后查看状态和kubectl一样需要带 -nNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSIONredis-ha-1591230205 redis 1 2020-06-04 08:23:33.325156857 +0800 CST deployed redis-ha-4.3.1 5.0.6 使用123456789101112131415# 查看部署后的pod[root@k8s1 redis-cluters]# kubectl get all -n redisNAME READY STATUS RESTARTS AGEpod/redis-ha-1591230205-server-0 2/2 Running 0 6h27mpod/redis-ha-1591230205-server-1 2/2 Running 0 6h26mpod/redis-ha-1591230205-server-2 2/2 Running 0 6h26mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/redis-ha-1591230205 ClusterIP None &lt;none&gt; 6379/TCP,26379/TCP 6h27mservice/redis-ha-1591230205-announce-0 ClusterIP 10.96.123.33 &lt;none&gt; 6379/TCP,26379/TCP 6h27mservice/redis-ha-1591230205-announce-1 ClusterIP 10.96.156.241 &lt;none&gt; 6379/TCP,26379/TCP 6h27mservice/redis-ha-1591230205-announce-2 ClusterIP 10.96.122.29 &lt;none&gt; 6379/TCP,26379/TCP 6h27mNAME READY AGEstatefulset.apps/redis-ha-1591230205-server 3/3 6h27m","link":"/2020/06/04/Redis%E5%AD%A6%E4%B9%A0/"},{"title":"Hexo记录","text":"Hexo 常用命令123hexo new post “新blog名” 创建新bloghexo clean 清除缓存hexo d -g 生成及部署至github 文内图片引用 站点配置文件，关键字：post_asset_folder 1post_asset_folder: true 执行新建命令时会创建同名文件夹 1$ hexo n &lt;title&gt; 将图片放入同名文件夹下，文章内图片相对引用 1{% asset_img 图片名称.jpg 图片描述 %} 例子​ 文内超链接 文内超链接 1[TextUrl](https://blog.jyusun.com/) 开启版权声明 主题配置文件下，关键字 creative_commons creative_commons:license: by-nc-sasidebar: truepost: truelanguage: zh-CN 站点配置文件, 关键字 url 修改为你自己的地址 1url: http://yoursite.com","link":"/2018/06/22/Hexo%E8%AE%B0%E5%BD%95/"},{"title":"Superviso详解","text":"https://mp.weixin.qq.com/s/MLTpgU_xCvATh9j7gqobNg","link":"/2020/06/09/Superviso%E8%AF%A6%E8%A7%A3/"},{"title":"Ubunt配置Sftp-ssh免密码登录","text":"Ubunt配置Sftp-ssh免密码登录之前都是用的Ftp，今天有两个客户要求用Sftp没用过，今天研究了一下，记录备忘SFTP其实就是加密的FTP 可以通过ssh密匙 免密码连接 系统：Ubuntu 16.04 软件：openssh 12345678910安装opensshsudo apt-get install openssh-server为SFTP访问创建用户组，便于管理权限sudo addgroup sftp-users创建用户加入组，不允许登录系统sudo adduser alice sudo usermod -G sftp-users -s /bin/false alice创建SSH用户组，并把管理员加入到该组（注意usermod中的-a参数的意思是不从其他用户组用移除）sudo addgroup ssh-users sudo usermod -a -G ssh-users root 准备“监狱”的根目录及共享目录。这里解释一下，“监狱”的根目录必须满足以下要求：所有者为root，其他任何用户都不能拥有写入权限。因此，为了让SFTP用户能够上传文件，还必须在“监狱”根目录下再创建一个普通用户能够写入的共享文件目录。为了便于管理员通过SFTP管理上传的文件，我把这个共享文件目录配置为：由admin所有，允许sftp-users读写。这样，管理员和SFTP用户组成员就都能读写这个目录了。 1234sudo mkdir /home/sftp_rootsudo mkdir /home/sftp_root/sharedsudo chown root:sftp-users /home/sftp_root/sharedsudo chmod 770 /home/sftp_root/shared 修改SSH配置文件1234567891011121314151617sudo vi/etc/ssh/sshd_config文件底部添加AllowGroups ssh-users sftp-usersMatch User sftp-ly # 针对用户设置额外配置 ChrootDirectory /sftp/sftp_root_ly/ # 根目录 AllowTcpForwarding no X11Forwarding no ForceCommand internal-sftpMatch User sftp-siemens ChrootDirectory /sftp/sftp_root_siemens/ AllowTcpForwarding no X11Forwarding no ForceCommand internal-sftp# 针对组设置Match Group sftp-users ....... 只允许ssh-uers及sftp-users通过SSH访问系统；可以针对用户或组，额外增加一些设置：将“/home/sftp_root”设置为该组用户的系统根目录（因此它们将不能访问该目录之外的其他系统文件）；禁止TCP Forwarding和X11 Forwarding；强制该组用户仅仅使用SFTP 到这sftp就算配置完成了 验证 linux随便找一台linux主机1sftp root@172.16.1.100 如果提示输密码，连接成功可以上传文件 表示成功！ 配置ssh免密登录 生成密匙在服务器上执行一下命令生成密匙123ssh-keygen -t rsa默认生成的路径是 /root/.ssh id_rsa 是私钥 一般用于客户端 id_rsa.pub 是公钥 一般用于服务器端 服务器上注册公钥12在/root/.ssh 目录下cat id_rsa.pub &gt;&gt; authorized_keys 将私钥放到客户端 linux1scp id_rsa root@172.16.1.11:/root/.ssh 也可以直接vim打开后复制私钥后在客户端.ssh文件夹下建立同名文件黏贴私钥 windows直接通过复制黏贴获取私钥 id_rsa 免密连接SFTP linux 1sftp root@172.16.1.100 不用输密码能直接连接成功表示成功 Windows打开WinSCP客户端-编辑站点-高级-验证-添加私钥[image:993226DC-0542-4C8F-90F4-03793171A0A6-500-0000112F79256455/816F6B62-A318-4ADF-B189-9809BE1B56ED.png][image:7B37C2E1-3884-411A-B350-7881B51EC6C1-500-0000113291CE8C69/F51BD6D3-01DE-475C-94EF-25B6BB7B62CA.png]添加密匙文件根据提示 将openssh格式的密匙转换为putty格式 ！！！不用输入密码，能连接成功！！成功啦！ 服务器端提供多用户服务如果要在SFTP服务器上建立多个用户 分别提供服务只需要在 相应用户的 /home/用户/.ssh 下放置正确的公匙并注册即可用相应的私钥 来连接 一个共享提供多个客户端连接一个sftp用户共享提供多个客户端连接只需要在相应用户的.ssh文件下的 authorized_keys 文件夹内添加多个公钥即可1cat id_rsa.pub &gt;&gt; authorized_keys 多来几次就行","link":"/2017/09/21/Ubunt%E9%85%8D%E7%BD%AESftp-ssh%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95/"},{"title":"hexo主题美化","text":"主题美化 icarus https://github.com/ppoffice/hexo-theme-icarus 魔改版 https://github.com/AlphaLxy/hexo-theme-icarus 目前使用中","link":"/2020/06/09/hexo%E4%B8%BB%E9%A2%98%E7%BE%8E%E5%8C%96/"},{"title":"k8s下部署zookeeper群集","text":"官网 https://zookeeper.apache.org/ 说明 zookeeper是一个服务注册中心,用于微服务架构,比较简单易用 说明 本次群集使用statfulset部署三节点集群 安装12345[root@k8s1 ~]# docker search zookeeperNAME DESCRIPTION STARS OFFICIAL AUTOMATEDzookeeper Apache ZooKeeper is an open-source server wh…# 使用的官方镜像,上传至harhub后使用,官方镜像的zk版本是最新的 下面的yaml文件,使用前先创建namespace 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248# zookeeper.yamlapiVersion: v1kind: Servicemetadata: labels: app: zk1 name: zk1-svc namespace: zookeeperspec: clusterIP: None ports: - port: 2181 protocol: TCP targetPort: 2181 name: client - port: 2888 protocol: TCP targetPort: 2888 name: leader - port: 3888 protocol: TCP targetPort: 3888 name: leader-election selector: app: zk1 sessionAffinity: None type: ClusterIP---apiVersion: v1kind: Servicemetadata: labels: app: zk2 name: zk2-svc namespace: zookeeperspec: clusterIP: None ports: - port: 2181 protocol: TCP targetPort: 2181 name: client - port: 2888 protocol: TCP targetPort: 2888 name: leader - port: 3888 protocol: TCP targetPort: 3888 name: leader-election selector: app: zk2 sessionAffinity: None type: ClusterIP---apiVersion: v1kind: Servicemetadata: labels: app: zk3 name: zk3-svc namespace: zookeeperspec: clusterIP: None ports: - port: 2181 protocol: TCP targetPort: 2181 name: client - port: 2888 protocol: TCP targetPort: 2888 name: leader - port: 3888 protocol: TCP targetPort: 3888 name: leader-election selector: app: zk3 sessionAffinity: None type: ClusterIP---apiVersion: v1kind: Servicemetadata: name: zk-cs namespace: zookeeper labels: app: zkspec: ports: - port: 2181 name: client selector: appp: zoo---apiVersion: apps/v1kind: StatefulSetmetadata: name: zk1 namespace: zookeeperspec: selector: matchLabels: app: zk1 serviceName: zk1-svc replicas: 1 template: metadata: labels: app: zk1 appp: zoo spec: containers: - name: kubernetes-zookeeper imagePullPolicy: Always image: \"172.16.130.72/basis/zookeeper\" ports: - containerPort: 2181 name: client - containerPort: 2888 name: server - containerPort: 3888 name: leader-election env: - name: ZOO_MY_ID value: \"1\" - name: ZOO_SERVERS value: server.1=zk1-svc:2888:3888;2181 server.2=zk2-svc:2888:3888;2181 server.3=zk3-svc:2888:3888;2181 - name: ZOO_INIT_LIMIT value: \"10\" - name: ZOO_SYNC_LIMIT value: \"5\" volumeMounts: - name: datadir mountPath: /data volumeClaimTemplates: - metadata: name: datadir spec: volumeMode: Filesystem storageClassName: nfs accessModes: [ \"ReadWriteMany\" ] resources: requests: storage: 2G---apiVersion: apps/v1kind: StatefulSetmetadata: name: zk2 namespace: zookeeperspec: selector: matchLabels: app: zk2 serviceName: zk2-svc replicas: 1 template: metadata: labels: app: zk2 appp: zoo spec: containers: - name: kubernetes-zookeeper imagePullPolicy: Always image: \"172.16.130.72/basis/zookeeper\" ports: - containerPort: 2181 name: client - containerPort: 2888 name: server - containerPort: 3888 name: leader-election env: - name: ZOO_MY_ID value: \"2\" - name: ZOO_SERVERS value: server.1=zk1-svc:2888:3888;2181 server.2=zk2-svc:2888:3888;2181 server.3=zk3-svc:2888:3888;2181 - name: ZOO_INIT_LIMIT value: \"10\" - name: ZOO_SYNC_LIMIT value: \"5\" volumeMounts: - name: datadir mountPath: /data volumeClaimTemplates: - metadata: name: datadir spec: volumeMode: Filesystem storageClassName: nfs accessModes: [ \"ReadWriteMany\" ] resources: requests: storage: 2G---apiVersion: apps/v1kind: StatefulSetmetadata: name: zk3 namespace: zookeeperspec: selector: matchLabels: app: zk3 serviceName: zk3-svc replicas: 1 template: metadata: labels: app: zk3 appp: zoo spec: containers: - name: kubernetes-zookeeper imagePullPolicy: Always image: \"172.16.130.72/basis/zookeeper\" ports: - containerPort: 2181 name: client - containerPort: 2888 name: server - containerPort: 3888 name: leader-election env: - name: ZOO_MY_ID value: \"3\" - name: ZOO_SERVERS value: server.1=zk1-svc:2888:3888;2181 server.2=zk2-svc:2888:3888;2181 server.3=zk3-svc:2888:3888;2181 - name: ZOO_INIT_LIMIT value: \"10\" - name: ZOO_SYNC_LIMIT value: \"5\" volumeMounts: - name: datadir mountPath: /data volumeClaimTemplates: - metadata: name: datadir spec: volumeMode: Filesystem storageClassName: nfs accessModes: [ \"ReadWriteMany\" ] resources: requests: storage: 2G 使用12# 验证 群集是三节点的,一主两从,如下图说明正常for i in 1 2 3; do kubectl exec --namespace zookeeper3 zk$i-0 zkServer.sh status; done 群集初始化可能有点问题,pod会反复重启,这时候需要手动删除一下pod,随便删,知道所有pod都runing,并且不再重启,群集初次选举成功后,再删除任意节点都不会影响群集可用性,可以手动删除进行测试. 1service/zk-cs 就是群集的群集IP服务,端口是2181 123456789# 随便进入一个zk podkubectl exec -it -n zookeeper3 zk1-0 /bin/bashroot@zk1-0:/apache-zookeeper-3.5.6-bin# zkCli.sh[zk: localhost:2181(CONNECTED) 0]connect 10.96.49.142:2181 #连接群集[zk: zk-cs:2181(CONNECTED) 1]connect zk-cs:2181 # 使用service的dns名称也可以# 连接成功后如下[zk: 10.96.49.142:2181(CONNECTED) 1]create /test 1 # 测试创建一个zone[zk: 10.96.49.142:2181(CONNECTED) 2] get /test # 测试取刚才创建的zone1","link":"/2020/01/17/k8s%E4%B8%8B%E9%83%A8%E7%BD%B2zookeeper%E7%BE%A4%E9%9B%86/"},{"title":"zabbix监控-ESXI-SNMP","text":"zabbix-监控-ESXI-SNMP参考:https://www.cnblogs.com/itfat/p/8044409.html 开启ESXI的SSH(通过ssh登录esxi后台) esxcli system snmp set --communities public public 为团体名 esxcli system snmp set --enable true ​ 开启snmp服务 zabbix创建主机 按下图配置 ​ ​","link":"/2020/06/02/zabbix%E7%9B%91%E6%8E%A7-ESXI-SNMP/"},{"title":"一文看懂traefik","text":"官网 https://containo.us/traefik/ 参考 https://www.qikqiak.com/post/traefik-2.1-101/ 说明 Traefik 是一个边缘路由器，是你整个平台的大门，拦截并路由每个传入的请求 说明1thumbnail: 3.png 相比nginx-ingress配置要简单一点 有一个web界面看起来舒服一点 安装安装直接参考上述连接,只修改namespace字段,单独放置. 使用暴露服务12345678910111213apiVersion: traefik.containo.us/v1alpha1kind: IngressRoutemetadata: name: simpleingressroute #名字随便取spec: entryPoints: - web #监控80端口 routes: - match: Host(`test.rokin.cn`) #监听的域名 kind: Rule services: - name: web-whoami #暴露的service port: 80 # 服务的端口 如果不指定namespace就是默认default,如果要暴露其他ns下的svc,需要指定namespace字段 IngressRoute如果创建,并且后端的services存在的话在界面上才会显示相应信息 将相关域名解析到k8s节点的80和443端口上,就可以访问了 https1234567891011121314151617181920# 创建secret, .cer也可以,secret也是ns下的kubectl create secret tls who-tls --cert=tls.crt --key=tls.keysecret/who-tls created# 修改或创建IngressRouteapiVersion: traefik.containo.us/v1alpha1kind: IngressRoutemetadata: name: ingressroutetlsspec: entryPoints: - websecure routes: - match: Host(`who.qikqiak.com`) &amp;&amp; PathPrefix(`/tls`) kind: Rule services: - name: whoami port: 80 tls: # 添加 secretName: who-tls # 添加 这就是上面创建的secret 中间层中间层是traefik的特色,可以实现很多功能,比如http自动跳转https,灰度发布,流量复制,等等,详见官网或参考. http自动跳转https12345678# 创建Middleware资源,也是ns下的apiVersion: traefik.containo.us/v1alpha1kind: Middlewaremetadata: name: redirect-httpsspec: redirectScheme: scheme: https 123456789101112131415apiVersion: traefik.containo.us/v1alpha1kind: IngressRoutemetadata: name: ingressroutetls-httpspec: entryPoints: - web routes: - match: Host(`who.qikqiak.com`) kind: Rule services: - name: whoami port: 80 middlewares: # 添加 - name: redirect-https # 添加 这样就会自动跳转到 who.qikqiak.com 的443上去 TCP就是通过域名暴露比如redis 6379这种端口出去,参考文档中有项目说明,下图需要重点注意","link":"/2020/01/17/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82traefik/"},{"title":"部署Prometheus Operator","text":"项目地址 https://github.com/coreos/kube-prometheus 参考地址 https://www.qikqiak.com/post/first-use-prometheus-operator/ 官网 https://prometheus.io/ 安装克隆项目123456789git clone https://github.com/coreos/kube-prometheus.git先运行这个,等pod都runingkubectl create -f manifests/setup再执行这个等所有pod runingkubectl create -f manifests/ 访问界面12345678910111213使用traefik暴露# prometheusservice: prometheus-k8s 9090# Grafana 默认密码: admin/adminservice: grafana 3000# Alert Managerservice: alertmanager-main 9093 数据持久化123456789101112131415161718192021222324252627282930执行 kubectl create -f manifests/ 前修改 /root/yaml/prometheus-operator/kube-prometheus/manifests/prometheus-prometheus.yamlapiVersion: monitoring.coreos.com/v1kind: Prometheusmetadata: labels: prometheus: k8s name: k8s namespace: monitoringspec: alerting: alertmanagers: - name: alertmanager-main namespace: monitoring port: web storage: volumeClaimTemplate: spec: storageClassName: nfs resources: requests: storage: 10Gi baseImage: quay.io/prometheus/prometheus nodeSelector:# 或者部署之后直接修改这段再 apply也行 配置配置告警信息1234567891011121314151617181920212223242526272829303132# alertmanager.yamlglobal: resolve_timeout: 5m smtp_smarthost: 'smtp.163.com:25' smtp_from: 'Rokin_NetGain@163.com' smtp_auth_username: 'Rokin_NetGain@163.com' smtp_auth_password: 'q59163303' smtp_hello: '163.com' smtp_require_tls: falseroute: group_by: ['job', 'severity'] group_wait: 30s group_interval: 5m repeat_interval: 12h receiver: default routes: - receiver: webhook match: alertname: CoreDNSDownreceivers:- name: 'default' email_configs: - to: '12345678@qq.com' send_resolved: true- name: 'webhook' webhook_configs: - url: 'http://dingtalk-hook.kube-ops:5000' send_resolved: true kubectl delete secrets -n monitoring alertmanager-mainkubectl create secret generic alertmanager-main --from-file=alertmanager.yaml -n monitoring 替换 secrets alertmanager-main 替换完后,界面上信息也会变掉","link":"/2020/01/17/%E9%83%A8%E7%BD%B2Prometheus-Operator/"}],"tags":[{"name":"部署","slug":"部署","link":"/tags/%E9%83%A8%E7%BD%B2/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"k8s","slug":"k8s","link":"/tags/k8s/"},{"name":"zookeeper","slug":"zookeeper","link":"/tags/zookeeper/"},{"name":"traefik","slug":"traefik","link":"/tags/traefik/"},{"name":"Prometheus","slug":"Prometheus","link":"/tags/Prometheus/"}],"categories":[{"name":"django","slug":"django","link":"/categories/django/"},{"name":"redis","slug":"redis","link":"/categories/redis/"},{"name":"运维","slug":"运维","link":"/categories/%E8%BF%90%E7%BB%B4/"},{"name":"k8s","slug":"k8s","link":"/categories/k8s/"}]}